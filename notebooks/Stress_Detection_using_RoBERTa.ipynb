{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs"
      ],
      "metadata": {
        "id": "jXMMh5u9Yi5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl emoji transformers torch datasets scikit-learn fsspec==2025.3.0 gcsfs==2025.3.0"
      ],
      "metadata": {
        "id": "pZvsvhnOY95r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFqLhRvpwy-2"
      },
      "outputs": [],
      "source": [
        "#one\n",
        "# Install required libraries\n",
        "#!pip install pandas openpyxl emoji transformers torch datasets scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H1MpGJIxCQp"
      },
      "outputs": [],
      "source": [
        "#two\n",
        "!git clone https://github.com/SenticNet/stress-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ITN0Mi_xFR6"
      },
      "outputs": [],
      "source": [
        "#three\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZizdQc2lDnqs"
      },
      "outputs": [],
      "source": [
        "#BERT on Reddit_Title.xlsx\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "df_title = pd.read_excel(\"/content/stress-detection/Reddit_Title.xlsx\")\n",
        "df_combi = pd.read_excel(\"/content/stress-detection/Reddit_Combi.xlsx\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = emoji.demojize(text)  # Convert emojis to text\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "        text = text.strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_title[\"text\"] = df_title[\"title\"].apply(preprocess_text)\n",
        "df_combi[\"text\"] = df_combi[\"Body_Title\"].apply(preprocess_text)\n",
        "\n",
        "# Rename labels (assuming dataset has \"label\" column where 1 = Stress, 0 = No Stress)\n",
        "df_title = df_title.rename(columns={\"label\": \"labels\"})\n",
        "df_combi = df_combi.rename(columns={\"label\": \"labels\"})\n",
        "\n",
        "# Select PLM model: Choose \"bert-base-uncased\", \"distilbert-base-uncased\", or \"roberta-base\"\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Convert Pandas to Hugging Face Dataset\n",
        "dataset_title = Dataset.from_pandas(df_title[[\"text\", \"labels\"]])\n",
        "dataset_combi = Dataset.from_pandas(df_combi[[\"text\", \"labels\"]])\n",
        "\n",
        "# Tokenize data\n",
        "dataset_title = dataset_title.map(tokenize_function, batched=True)\n",
        "dataset_combi = dataset_combi.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split into train & test sets (80% train, 20% test)\n",
        "dataset_title = dataset_title.train_test_split(test_size=0.2)\n",
        "dataset_combi = dataset_combi.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define evaluation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_title[\"train\"],\n",
        "    eval_dataset=dataset_title[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X67xURyBxIDG"
      },
      "outputs": [],
      "source": [
        "#four\n",
        "#RoBERT on Reddit_Title.xlsx\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "df_title = pd.read_excel(\"/content/stress-detection/Reddit_Title.xlsx\")\n",
        "df_combi = pd.read_excel(\"/content/stress-detection/Reddit_Combi.xlsx\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = emoji.demojize(text)  # Convert emojis to text\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "        text = text.strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_title[\"text\"] = df_title[\"title\"].apply(preprocess_text)\n",
        "df_combi[\"text\"] = df_combi[\"Body_Title\"].apply(preprocess_text)\n",
        "\n",
        "# Rename labels (assuming dataset has \"label\" column where 1 = Stress, 0 = No Stress)\n",
        "df_title = df_title.rename(columns={\"label\": \"labels\"})\n",
        "df_combi = df_combi.rename(columns={\"label\": \"labels\"})\n",
        "\n",
        "# Select PLM model: Choose \"bert-base-uncased\", \"distilbert-base-uncased\", or \"roberta-base\"\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Convert Pandas to Hugging Face Dataset\n",
        "dataset_title = Dataset.from_pandas(df_title[[\"text\", \"labels\"]])\n",
        "dataset_combi = Dataset.from_pandas(df_combi[[\"text\", \"labels\"]])\n",
        "\n",
        "# Tokenize data\n",
        "dataset_title = dataset_title.map(tokenize_function, batched=True)\n",
        "dataset_combi = dataset_combi.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split into train & test sets (80% train, 20% test)\n",
        "dataset_title = dataset_title.train_test_split(test_size=0.2)\n",
        "dataset_combi = dataset_combi.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define evaluation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_title[\"train\"],\n",
        "    eval_dataset=dataset_title[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrkvvXuPb9tK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Get predictions\n",
        "predictions = trainer.predict(dataset_title['test'])\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L5oaJgUaWE8"
      },
      "outputs": [],
      "source": [
        "#five\n",
        "!pip install joblib\n",
        "import joblib\n",
        "\n",
        "# Save model and tokenizer\n",
        "MODEL_SAVE_PATH = \"./roberta_stress_model.pkl\"\n",
        "TOKENIZER_SAVE_PATH = \"./roberta_tokenizer.pkl\"\n",
        "\n",
        "joblib.dump(model, MODEL_SAVE_PATH)\n",
        "joblib.dump(tokenizer, TOKENIZER_SAVE_PATH)\n",
        "\n",
        "print(\"Model and tokenizer saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FAyLFcHbv7Q"
      },
      "outputs": [],
      "source": [
        "#six\n",
        "!pip install streamlit==1.41.1 pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import streamlit as st\n",
        "import torch\n",
        "import joblib\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "import emoji\n",
        "import time\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model and tokenizer (Ensure correct path in Google Drive or upload manually)\n",
        "MODEL_PATH = \"./roberta_stress_model.pkl\"\n",
        "TOKENIZER_PATH = \"./roberta_tokenizer.pkl\"\n",
        "\n",
        "try:\n",
        "    model = joblib.load(MODEL_PATH).to(device)\n",
        "    tokenizer = joblib.load(TOKENIZER_PATH)\n",
        "except:\n",
        "    st.error(\"Model or tokenizer not found! Please upload them.\")\n",
        "    st.stop()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = emoji.demojize(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Prediction function\n",
        "def predict_stress(text):\n",
        "    processed_text = preprocess_text(text)\n",
        "    inputs = tokenizer(processed_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    stress_score = probabilities[0][1].item()\n",
        "    return stress_score\n",
        "\n",
        "# Streamlit UI\n",
        "st.set_page_config(page_title=\"Stress Detection\", layout=\"centered\")\n",
        "st.markdown(\"\"\"\n",
        "    <h1 style='text-align: center; color: #ff4b4b;'>🧠 Stress Detection from Reddit Titles</h1>\n",
        "    <p style='text-align: center;'>Enter a Reddit post title to check if it indicates stress.</p>\n",
        "    <hr>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "user_input = st.text_area(\"Enter Reddit Title:\", \"\", help=\"Type a Reddit post title here...\")\n",
        "\n",
        "if st.button(\"Analyze\", use_container_width=True):\n",
        "    if user_input:\n",
        "        with st.spinner(\"Analyzing...\"):\n",
        "            time.sleep(1)  # Simulating processing time\n",
        "            stress_score = predict_stress(user_input)\n",
        "            label = \"😨 Stressed\" if stress_score > 0.5 else \"😊 Not Stressed\"\n",
        "\n",
        "            st.subheader(f\"Prediction: {label}\")\n",
        "            st.progress(stress_score)\n",
        "            st.write(f\"Confidence Score: {stress_score:.4f}\")\n",
        "    else:\n",
        "        st.warning(\"⚠️ Please enter a Reddit title.\")\n",
        "\n",
        "st.markdown(\"<p style='text-align: center; color: gray;'>Powered by <b>RoBERTa</b> for stress detection.</p>\", unsafe_allow_html=True)\n",
        "'''"
      ],
      "metadata": {
        "id": "36X8H5MmXRd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtXGHeG5b0cD"
      },
      "outputs": [],
      "source": [
        "#seven\n",
        "#replace the [authtoken]\n",
        "!ngrok config add-authtoken [authtoken]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLaAT2Seb3Rl"
      },
      "outputs": [],
      "source": [
        "#eight\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Define the new port for the Streamlit app\n",
        "new_port = 8508\n",
        "\n",
        "# Run the Streamlit app on the new port\n",
        "!streamlit run appr.py --server.port {new_port} &>/content/logs.txt &\n",
        "\n",
        "# Ensure all previous tunnels are closed before opening a new one\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new ngrok tunnel on the new port\n",
        "public_url = ngrok.connect(new_port, \"http\")\n",
        "print(f\"Public URL: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JJUtdx_b5_i"
      },
      "outputs": [],
      "source": [
        "# Ensure all previous tunnels are closed before opening a new one\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6UB2Px6OQuo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5dsevvhOQi4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx6NN0rz3gLy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "# Function to preprocess text (same as used during training)\n",
        "def preprocess_text(text):\n",
        "    text = emoji.demojize(text)  # Convert emojis to text\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Function to predict stress from user input\n",
        "def predict_stress(text):\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Move input to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Return result\n",
        "    return \"Stress Detected 😟\" if prediction == 1 else \"No Stress Detected 😊\"\n",
        "\n",
        "# Continuous loop to take input from the user\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Exiting stress detection system.\")\n",
        "        break\n",
        "    result = predict_stress(user_input)\n",
        "    print(f\"Prediction: {result}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TTrv-ou39uv"
      },
      "outputs": [],
      "source": [
        "#RoBERT on Reddit_Combi.xlsx\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "df_title = pd.read_excel(\"/content/stress-detection/Reddit_Title.xlsx\")\n",
        "df_combi = pd.read_excel(\"/content/stress-detection/Reddit_Combi.xlsx\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = emoji.demojize(text)  # Convert emojis to text\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "        text = text.strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_title[\"text\"] = df_title[\"title\"].apply(preprocess_text)\n",
        "df_combi[\"text\"] = df_combi[\"Body_Title\"].apply(preprocess_text)\n",
        "\n",
        "# Rename labels (assuming dataset has \"label\" column where 1 = Stress, 0 = No Stress)\n",
        "df_title = df_title.rename(columns={\"label\": \"labels\"})\n",
        "df_combi = df_combi.rename(columns={\"label\": \"labels\"})\n",
        "\n",
        "# Select PLM model: Choose \"bert-base-uncased\", \"distilbert-base-uncased\", or \"roberta-base\"\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Convert Pandas to Hugging Face Dataset\n",
        "dataset_title = Dataset.from_pandas(df_title[[\"text\", \"labels\"]])\n",
        "dataset_combi = Dataset.from_pandas(df_combi[[\"text\", \"labels\"]])\n",
        "\n",
        "# Tokenize data\n",
        "dataset_title = dataset_title.map(tokenize_function, batched=True)\n",
        "dataset_combi = dataset_combi.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split into train & test sets (80% train, 20% test)\n",
        "dataset_title = dataset_title.train_test_split(test_size=0.2)\n",
        "dataset_combi = dataset_combi.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define evaluation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_combi[\"train\"],\n",
        "    eval_dataset=dataset_combi[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSXnT-7W82aM"
      },
      "outputs": [],
      "source": [
        "#distilbert on Reddit_Title.xlsx\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "df_title = pd.read_excel(\"/content/stress-detection/Reddit_Title.xlsx\")\n",
        "df_combi = pd.read_excel(\"/content/stress-detection/Reddit_Combi.xlsx\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = emoji.demojize(text)  # Convert emojis to text\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "        text = text.strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_title[\"text\"] = df_title[\"title\"].apply(preprocess_text)\n",
        "df_combi[\"text\"] = df_combi[\"Body_Title\"].apply(preprocess_text)\n",
        "\n",
        "# Rename labels (assuming dataset has \"label\" column where 1 = Stress, 0 = No Stress)\n",
        "df_title = df_title.rename(columns={\"label\": \"labels\"})\n",
        "df_combi = df_combi.rename(columns={\"label\": \"labels\"})\n",
        "\n",
        "# Select PLM model: Choose \"bert-base-uncased\", \"distilbert-base-uncased\", or \"roberta-base\"\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Convert Pandas to Hugging Face Dataset\n",
        "dataset_title = Dataset.from_pandas(df_title[[\"text\", \"labels\"]])\n",
        "dataset_combi = Dataset.from_pandas(df_combi[[\"text\", \"labels\"]])\n",
        "\n",
        "# Tokenize data\n",
        "dataset_title = dataset_title.map(tokenize_function, batched=True)\n",
        "dataset_combi = dataset_combi.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split into train & test sets (80% train, 20% test)\n",
        "dataset_title = dataset_title.train_test_split(test_size=0.2)\n",
        "dataset_combi = dataset_combi.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define evaluation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_title[\"train\"],\n",
        "    eval_dataset=dataset_title[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re0W-BHqAagu"
      },
      "outputs": [],
      "source": [
        "#distilbert on Reddit_Combi.xlsx\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "df_title = pd.read_excel(\"/content/stress-detection/Reddit_Title.xlsx\")\n",
        "df_combi = pd.read_excel(\"/content/stress-detection/Reddit_Combi.xlsx\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = emoji.demojize(text)  # Convert emojis to text\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "        text = text.strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_title[\"text\"] = df_title[\"title\"].apply(preprocess_text)\n",
        "df_combi[\"text\"] = df_combi[\"Body_Title\"].apply(preprocess_text)\n",
        "\n",
        "# Rename labels (assuming dataset has \"label\" column where 1 = Stress, 0 = No Stress)\n",
        "df_title = df_title.rename(columns={\"label\": \"labels\"})\n",
        "df_combi = df_combi.rename(columns={\"label\": \"labels\"})\n",
        "\n",
        "# Select PLM model: Choose \"bert-base-uncased\", \"distilbert-base-uncased\", or \"roberta-base\"\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Convert Pandas to Hugging Face Dataset\n",
        "dataset_title = Dataset.from_pandas(df_title[[\"text\", \"labels\"]])\n",
        "dataset_combi = Dataset.from_pandas(df_combi[[\"text\", \"labels\"]])\n",
        "\n",
        "# Tokenize data\n",
        "dataset_title = dataset_title.map(tokenize_function, batched=True)\n",
        "dataset_combi = dataset_combi.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split into train & test sets (80% train, 20% test)\n",
        "dataset_title = dataset_title.train_test_split(test_size=0.2)\n",
        "dataset_combi = dataset_combi.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define evaluation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_combi[\"train\"],\n",
        "    eval_dataset=dataset_combi[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vayiw-GhKAlx"
      },
      "outputs": [],
      "source": [
        "#EMOTION CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaoMjTn1Guc2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "df_combi = pd.read_excel(\"/content/stress-detection/Reddit_Combi.xlsx\")\n",
        "\n",
        "# Print column names to verify structure\n",
        "print(\"Dataset Columns:\", df_combi.columns)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = emoji.demojize(text)  # Convert emojis to text\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "        text = text.strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_combi[\"text\"] = df_combi[\"Body_Title\"].apply(preprocess_text)\n",
        "\n",
        "# Emotion label mapping (Modify as needed)\n",
        "emotion_labels = {\n",
        "    0: \"joy\",\n",
        "    1: \"sadness\",\n",
        "    2: \"anger\",\n",
        "    3: \"fear\",\n",
        "    4: \"neutral\"\n",
        "}\n",
        "\n",
        "# Check if \"label\" column exists\n",
        "if \"label\" not in df_combi.columns:\n",
        "    raise KeyError(\"The 'label' column is missing. Check your dataset structure.\")\n",
        "\n",
        "# Map labels to numerical values\n",
        "df_combi[\"labels\"] = df_combi[\"label\"]\n",
        "\n",
        "# Drop rows with missing labels\n",
        "df_combi = df_combi.dropna(subset=[\"labels\"]).reset_index(drop=True)\n",
        "\n",
        "# Convert labels to integers\n",
        "df_combi[\"labels\"] = df_combi[\"labels\"].astype(int)\n",
        "\n",
        "# Select Pretrained Model\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(emotion_labels))\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "dataset_combi = Dataset.from_pandas(df_combi[[\"text\", \"labels\"]])\n",
        "\n",
        "# Tokenize data\n",
        "dataset_combi = dataset_combi.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the \"text\" column (but NOT \"__index_level_0__\" since it doesn't exist)\n",
        "dataset_combi = dataset_combi.remove_columns([\"text\"])\n",
        "\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%)\n",
        "dataset_combi = dataset_combi.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./emotion_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Define evaluation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_combi[\"train\"],\n",
        "    eval_dataset=dataset_combi[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g0pvDSXWbXF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Metrics\n",
        "accuracy = [0.964, 0.964, 0.972]\n",
        "precision = [0.964, 0.965, 0.972]\n",
        "recall = [0.963, 0.964, 0.969]\n",
        "f1_score = [0.964, 0.964, 0.971]\n",
        "\n",
        "# Model names\n",
        "models = [\"BERT\", \"DistilBERT\", \"RoBERTa\"]\n",
        "metrics = [accuracy, precision, recall, f1_score]\n",
        "titles = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "\n",
        "# Bar width and color palette\n",
        "bar_width = 0.5\n",
        "colors = [\"#4C72B0\", \"#55A868\", \"#C44E52\"]\n",
        "\n",
        "# Create figure\n",
        "fig, axs = plt.subplots(4, 1, figsize=(8, 16))\n",
        "plt.suptitle(\"Performance Metrics of BERT, DistilBERT, and RoBERTa\", fontsize=14, fontweight='bold')\n",
        "\n",
        "# Function to create bar plots with labels\n",
        "def create_bar_plot(ax, data, title, color):\n",
        "    sns.barplot(x=models, y=data, ax=ax, hue=models, palette=color, dodge=False)  # Updated line\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylim(0.95, 1.0)\n",
        "    ax.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "    # (Rest of your function code remains the same)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(data):\n",
        "        ax.text(i, v + 0.002, f\"{v:.3f}\", ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "for i, ax in enumerate(axs):\n",
        "    ax.bar(models, metrics[i], color=['blue', 'green', 'red'])\n",
        "    ax.set_ylim(0.95, 1.00)\n",
        "    ax.set_title(titles[i], fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Display values on bars\n",
        "    for j, value in enumerate(metrics[i]):\n",
        "        ax.text(j, value + 0.002, f\"{value:.3f}\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}